[
  {
    "objectID": "posts/EasyTask/index.html",
    "href": "posts/EasyTask/index.html",
    "title": "EasyTask",
    "section": "",
    "text": "As a data analyst, working with a data set involves several important steps to gain insights and make informed decisions some important or crucial aspects are -\n\nData Set Summary\nData Distribution\nData Visualization\nModel building\n\nPython is a great tools to creating application easily lots of the framework are pre build to do that one of the important as respect to analyst is Streamlit.\nI am creating a developments application for my working purpose and I share with you and you moderate as your requirements otherwise drop a mail with changes I will help you.\nFive things in this application and plug in lots of useful things as per project requirements mainly this version it have:\n\nUpload CSV\nData view tab\nData Summary\nUnivariate and Bivariate Plots\nSimple and Multiple regression Models\n\n\nIts a better practice to create a folder and inside this folders create necessary files. In this project (EasyTask) have three files 1. requirements.txt 2. app.py 3. plot.py.\nOpen the terminal and go through the directory with write the command\n\npip install -r requirements.txt\n\nDownload all the necessary python module with packages for run this projects. It is the better processes before run jump into the code at first create a virtual environments and activate this environments.\nThen copy and past this code on the plot.py files\n\nimport streamlit as st\nimport matplotlib.pyplot as plt\n\ndef univariateplot(plot_type, df, variable_name):\n    \n    if plot_type == 'Line Plot':\n        plt.plot(df[variable_name])\n        plt.title('Line Plot')\n        st.pyplot()\n    elif plot_type == 'Bar Plot':\n        plt.bar(df.index, df[variable_name])\n        plt.title('Bar Plot')\n        st.pyplot()\n    elif plot_type == 'Scatter Plot':\n        plt.scatter(df.index, df[variable_name])\n        plt.title('Scatter Plot')\n        st.pyplot()\n    else:\n        st.write('Select a plot type.')\n\n\ndef bivariateplot(plot_type, df, variable1, variable2):\n    if plot_type == 'Line Plot':\n        plt.plot(df[variable1], df[variable2])\n        plt.title('Line Plot')\n        st.pyplot()\n    elif plot_type == 'Bar Plot':\n        plt.bar(df[variable1], df[variable2])\n        plt.title('Bar Plot')\n        st.pyplot()\n    elif plot_type == 'Scatter Plot':\n        plt.scatter(df[variable1], df[variable2])\n        plt.title('Scatter Plot')\n        st.pyplot()\n    else:\n        st.write('Select a plot type.')\n\nJust separate the type of plots in a .py files because it is easy to moderate as per requirements and it connect our app.py file.\n\nimport numpy as np\nimport pandas as pd\nimport streamlit as st\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom plot import univariateplot, bivariateplot\n\n\ndef main():\n    # st.set_page_config(layout=\"wide\")\n    st.set_option('deprecation.showPyplotGlobalUse', False)\n\n    data_input()\n    data_output()\n\ndef data_input():\n    # Use Streamlit widgets in the sidebar to get user input\n    uploaded_file = st.sidebar.file_uploader(\"Upload a CSV file\", type=[\"csv\"])\n    if uploaded_file is not None:\n        df = pd.read_csv(uploaded_file)\n        st.session_state['uploaded_df'] = df\n      \ndef data_output():\n    if 'uploaded_df' in st.session_state:\n        df = st.session_state['uploaded_df']\n        \n        tab1, tab2, tab3, tab4 = st.tabs(\n          [\"DATA\", \"SUMMARY\", \"PLOTS\", \"REGRESSION\"])\n        with tab1:\n            if 'uploaded_df' in st.session_state:\n                df = st.session_state['uploaded_df']\n                st.write(\"\")\n                st.write(\"\")\n                st.dataframe(df)\n        with tab2:\n            st.write(\"\")\n            st.write(\"\")\n            des = df.describe()\n            st.write(des)\n\n        with tab3:\n            if 'uploaded_df' in st.session_state:\n                df = st.session_state['uploaded_df']\n\n            variable_type = st.radio(\"Choose data type:\",\n                                     (\"Univariate\", \"Bivariate\"))\n            if variable_type == \"Univariate\":\n                plot_type = st.selectbox(\n                  \"Select Plot type:\",['Line Plot', 'Bar Plot', 'Scatter Plot'])\n                variable_name = st.selectbox(\"Select a variable:\", df.columns)\n\n                st.write(\"You selected:\", plot_type, \"based on\", variable_name)\n                if st.button('Create Plot'):\n                    univariateplot(plot_type, df, variable_name)\n\n            elif variable_type == \"Bivariate\":\n                plot_type = st.selectbox(\n                  \"Select Plot type:\", ['Line Plot', 'Bar Plot', 'Scatter Plot'])\n            \n                variable_name1 = st.selectbox(\n                  \"Select x axis variable:\", df.columns)\n                \n                variable_name2 = st.selectbox(\n                  \"Select y axis variable:\", df.columns)\n\n                st.write(\"You selected:\", plot_type, \"based on\",\n                         variable_name1, \"and\", variable_name2)\n                \n                if st.button('Create Plot'):\n                    bivariateplot(plot_type, df, variable_name1, variable_name2)\n\n        with tab4:\n            dependent_variable = st.selectbox(\n              \"Select dependent variable:\", df.columns, key= \"dependent_variable\")\n            independent_variable = st.selectbox(\n              \"Select independent variable:\",df.columns,key=\"independent_variable\")\n            \n            endog_data = df[dependent_variable]\n\n            exog_data = df[independent_variable]\n            exog_data = sm.add_constant(exog_data)\n\n            if st.button(\"Create Model\"):\n                model = sm.OLS(endog_data, exog_data)\n                res = model.fit()\n                st.write(res.summary())\n\n    else:\n        st.header(\"EasyTask\")\n        st.subheader(\"Introducing our revolutionary project pre-stage idea – a  seamless and user-friendly platform that transforms your CSV data into actionable insights with just a few clicks!\")\n        st.write(\"Thank you for considering me for your development needs. I am here to assist you throughout the development process. Please don't hesitate to reach out if you have any questions or require any modifications. Your satisfaction is my priority, and I am more than happy to help. Feel free to contact me anytime, and together, we can create something remarkable.\")\n        st.write(\"email id: maitysougata724@gmail.com\")\n\nif __name__ == \"__main__\":\n    main()\n\nFor a data analyst and data scientist its a best practice to doing this types of intermediate project it give an idea how functionality run and to connect all the components.\nPlots\n\n\nRegression Model\n\nAll the code share in my github page download and run. If you have any problems please let me know I am happy to help you."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Content with Code",
    "section": "",
    "text": "Dealing with missing data\n\n\nImputation, deletion, or machine learning are used to handle missing data, ensuring unbiased and accurate analyses.\n\n\n\n\nData Analysis\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nAug 2, 2023\n\n\nSougata Maity\n\n\n\n\n\n\n  \n\n\n\n\nProcess data using apply function\n\n\nUse apply function to process data row-wise or column-wise efficiently.\n\n\n\n\npython\n\n\nData analysis\n\n\nR Code\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2023\n\n\nSougata Maity\n\n\n\n\n\n\n  \n\n\n\n\nConfigure multiple SSH keys for different GitHub accounts in Git.\n\n\nGenerate two SSH keys, add them to the GitHub accounts, and set up ~/.ssh/config to associate each key with the corresponding account.\n\n\n\n\nData Science\n\n\ngithub\n\n\ngit\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2023\n\n\nSougata Maity\n\n\n\n\n\n\n  \n\n\n\n\nEasyTask\n\n\nCustom application transforms CSV data into actionable insights with just a few clicks\n\n\n\n\npython\n\n\nData Analysis\n\n\nStreamlit\n\n\n\n\n\n\n\n\n\n\n\nJul 28, 2023\n\n\nSougata Maity\n\n\n\n\n\n\n  \n\n\n\n\nTableau Lite\n\n\nCustom application using python streamlit and pygwalker\n\n\n\n\npython\n\n\nData analysis\n\n\n\n\n\n\n\n\n\n\n\n\nJul 28, 2023\n\n\nSougata Maity\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Tableau Lite/index.html",
    "href": "posts/Tableau Lite/index.html",
    "title": "Tableau Lite",
    "section": "",
    "text": "Tableau is used for data visualization and business intelligence, helping users analyze and explore data through interactive visualizations. Its user-friendly interface and data connectivity allow for easy exploration of various data sources, while dashboards and data stories enable effective communication of insights. With scalability and advanced analytics capabilities, Tableau is a powerful tool for organizations to make data-driven decisions and gain valuable insights from their data.\nIt a separate application to doing this but some times we are need inside our python application or jupyter notebook, streamlit or any other python application framework.\nAs a data analyst main work to do\n\nFinding results as per business problems using business data.\nSolving research problems in a study using capture data.\nCuriosity about finding inside in a data set or multiple data sets.\nPattern finding and model fitting based on the data.\n\nAll of them are easily recognized based on the graphical representation based on selected appropriate selecting feature (variable) and manipulation.\n\nStreamlit application\nStreamlit is a Python library used to create web applications for data science and machine learning projects. With just a few lines of code, developers can transform data scripts into interactive web apps. It simplifies the process of sharing and visualizing data, making it an efficient and user-friendly tool for data scientists and developers to showcase their work and engage with end-users interactively.\nHere we are creating a python streamlit application using pygwalker.\nIt is a good practice to creating separate for all the project and inside folder creating and storing all necessary files. For this project creating a folder tableauLite inside this folders crating two files app.py and requirements.txt inside requirements.txt file write pandas, streamlit and pygwalker.\nOpen anaconda prompt set this file location and run the code pip install -r requirements.txt (all files are upload my github page)\nNow open our main app.py files and copy past this code otherwise download this file in my github page to direct run the code.\n\nimport streamlit as st\nimport pandas as pd\nimport pygwalker as pyg\nimport streamlit.components.v1 as components\n\ndef main():\n    st.set_page_config(layout=\"wide\")\n    custom_css = \"\"\"\n    &lt;style&gt;\n    body {\n        margin-top: 1px;\n    }\n    &lt;/style&gt;\n    \"\"\"\n    st.markdown(custom_css, unsafe_allow_html=True)\n    data_input()\n    data_output()\n\ndef data_input():\n    # Use Streamlit widgets in the sidebar to get user input\n    uploaded_file = st.sidebar.file_uploader(\"Upload a CSV file\", type=[\"csv\"])\n\n    # Process the uploaded file if it exists\n    if uploaded_file is not None:\n        df = pd.read_csv(uploaded_file)\n        st.session_state['uploaded_df'] = df\n\ndef data_output():\n    if 'uploaded_df' in st.session_state:\n        df = st.session_state['uploaded_df']\n        pyg_html = pyg.walk(df, return_html=True)\n        components.html(pyg_html, height= 1000,  scrolling=True)\n   \n\nif __name__ == \"__main__\":\n    main()\n\nFinally we have necessary files now open teminal and run streamlit run app.py it will start a local web server and provide you with a local URL http://localhost:8501. Open the web browser and enter this URL to access our Streamlit application.\nIn general now this version two tabs one is Data and another is Visualization.\nVisualization Tab\n\nData Tab"
  },
  {
    "objectID": "posts/Dealing with missing data/index.html",
    "href": "posts/Dealing with missing data/index.html",
    "title": "Dealing with missing data",
    "section": "",
    "text": "Dealing with missing data is an essential step in data analysis and modeling. Missing data can arise due to various reasons, such as incomplete surveys, data entry errors, or sensor malfunctions. Handling missing data effectively is crucial to ensure the accuracy and reliability of your analysis. Here are some common approaches to deal with missing data:\n\nIdentify and Understand the Missing Data Pattern:\n\nStart by examining your dataset to identify the missing values and understand the pattern of missingness. Some common patterns include Missing Completely At Random (MCAR), Missing At Random (MAR), and Missing Not At Random (MNAR). Understanding the pattern can help you choose an appropriate strategy.\n\nRemove Rows with Missing Data (Listwise Deletion):\n\nOne simple approach is to remove entire rows that contain missing values. This method is called listwise deletion. However, this approach can lead to a loss of valuable information, especially if the missing data is not randomly distributed.\n\nImputation Techniques:\n\nImputation involves replacing missing values with estimated values based on the information available in the dataset. Common imputation methods include:\n\nMean/Median imputation: Replace missing values with the mean or median of the observed values in the respective column.\nMode imputation: For categorical variables, replace missing values with the mode (most frequently occurring value) of the observed values.\nRegression imputation: Use regression models to predict missing values based on other variables in the dataset.\nK-nearest neighbors (KNN) imputation: Replace missing values with the values from the K-nearest neighbors based on similarity measures.\n\n\nCreate Indicator Variables (Dummy Variables):\n\nIn some cases, it may be useful to create an additional binary indicator variable that denotes the presence or absence of a missing value in a particular column. This can help the model to treat missingness as a feature rather than losing information by imputation.\n\nData Transformation:\n\nIn some cases, transforming the data or using relative proportions can be helpful in handling missing data. For example, if you have a time series dataset with missing values, you can calculate the percentage change from the previous time point instead of using the raw values.\n\nModel-Based Methods:\n\nIf the missing data pattern is related to other variables in the dataset, you can use advanced statistical models that account for the missing data mechanism explicitly, such as multiple imputation or Maximum Likelihood Estimation (MLE).\n\nDomain Expertise:\n\nDepending on the context of your data, you might also leverage domain expertise to make reasonable assumptions and fill in missing values manually.\n\n\nRemember, the choice of method depends on the nature of your data, the extent of missingness, and the assumptions you can reasonably make. Additionally, it’s essential to be transparent about how you handle missing data and consider the potential impact on the interpretation of your results."
  },
  {
    "objectID": "posts/Process data using apply function/index.html",
    "href": "posts/Process data using apply function/index.html",
    "title": "Process data using apply function",
    "section": "",
    "text": "Apply function one of the most important to row or column manipulation to apply a function to either rows or columns of a matrix or data frame. The apply function is a powerful tool for performing operations across rows or columns efficiently.\nThe primary purpose of using a row-wise apply function is to perform element-wise operations or computations on individual rows of the data and generate new values or summaries based on the row-level data.\nHere are a few common use cases and benefits of using row-wise apply functions:\n\nElement-wise operations: Sometimes, you need to perform calculations or transformations on elements within a row independently of other rows. Using a row-wise apply function allows you to conveniently apply a function to each row, processing the elements in isolation.\nFeature engineering: When working with datasets, you might need to create new features based on existing ones. A row-wise apply function can be used to generate new columns in a DataFrame or matrix by applying a function to the values of each row.\nRow-wise aggregation: In data analysis, you may want to summarize or aggregate data at the row level, like calculating the mean, median, sum, or any custom function of multiple values within a row. Row-wise apply functions enable you to perform these operations efficiently.\nConditional computations: When dealing with complex data structures, you may need to apply different computations to different rows based on specific conditions. Row-wise apply functions can handle such cases by allowing you to implement custom logic for each row.\nParallel processing: Depending on the implementation, some row-wise apply functions can take advantage of parallel processing, which can significantly speed up computations on large datasets.\n\nWhen choosing to use a row-wise apply function, consider the size of your dataset, the complexity of the computation, and the available resources. As with any programming task, it’s essential to balance readability, maintainability, and performance.\n\n\nR-Code\nUsing apply for Row-wise sum\n\ndf &lt;- data.frame(\n  A = c(1, 2, 3),\n  B = c(4, 5, 6),\n  C = c(7, 8, 9)\n)\n\ndf\n\n  A B C\n1 1 4 7\n2 2 5 8\n3 3 6 9\n\n\n\ndf$rowsum &lt;- apply(df, 1, sum)\ndf\n\n  A B C rowsum\n1 1 4 7     12\n2 2 5 8     15\n3 3 6 9     18\n\n\nHere in this apply function use in build sum function but we use custom function also. We take a function mul2each sum (each number multiply with own plus 2).\n\nmul2each &lt;- function(x){\n  res = sum((x*x) + 2)\n  return(res)\n}\n\ndf2 &lt;- data.frame(\n  A = c(1, 2, 3),\n  B = c(4, 5, 6),\n  C = c(7, 8, 9)\n)\n\ndf2$result &lt;- apply(df2, 1, mul2each)\ndf2\n\n  A B C result\n1 1 4 7     72\n2 2 5 8     99\n3 3 6 9    132\n\n\nAs per requirements create a function and link this function inside the main apply function.\nUsing apply for Column-wise sum\n\ndf3 &lt;- data.frame(\n  A = c(1, 2, 3),\n  B = c(4, 5, 6),\n  C = c(7, 8, 9)\n)\n\ncolsum &lt;- apply(df, 2, sum)\ncolsum\n\n     A      B      C rowsum \n     6     15     24     45 \n\n\nSimilarly as per requirements create a custom function and use it for finding column manipulation.\n\n\nPython\nUsing apply for Row-wise sum\n\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'A': [1, 2, 3],\n    'B': [4, 5, 6],\n    'C': [7, 8, 9]\n})\ndf\n\n   A  B  C\n0  1  4  7\n1  2  5  8\n2  3  6  9\n\n\nCreating a custom function sum of (number *2 + 2)\n\n\ndef sum_of_squares(row):\n    return sum((row**2)+2)\n\n\nrow_sum = df.apply(sum_of_squares, axis=1)\nprint(row_sum)\n\n0     72\n1     99\n2    132\ndtype: int64\n\n\nUsing apply for Column-wise sum\n\ncolumn_sum = df.apply(sum_of_squares, axis=0)\nprint(column_sum)\n\nA     20\nB     83\nC    200\ndtype: int64"
  },
  {
    "objectID": "posts/Two github account connect with ssh/index.html",
    "href": "posts/Two github account connect with ssh/index.html",
    "title": "Configure multiple SSH keys for different GitHub accounts in Git.",
    "section": "",
    "text": "To connect SSH keys for two separate GitHub accounts (one personal and another professional) on your local desktop, we can follow these steps:\n\nGenerate SSH Keys: First, we need to generate SSH keys for each GitHub account. Open a terminal or command prompt on our local machine and run the following command, replacing “your_email@example.com” with the email associated with each GitHub account:\nssh-keygen -t ed25519 -C \"your_email@example.com\"\nThis command generates a new SSH key pair (public and private keys) for each email address you provide.\nSave SSH Keys: When prompted, we can choose the location to save the SSH keys. By default, the keys are saved in the ~/.ssh directory with names like id_ed25519 for the private key and id_ed25519.pub for the public key. Now we have two pairs of keys for each GitHub account.\nAdd SSH Keys to GitHub Accounts:\n\nLog in to your personal GitHub account in a web browser.\nGo to “Settings” &gt; “SSH and GPG keys”.\nClick on “New SSH key”.\nOpen the public key file id_ed25519.pub from your personal account’s SSH key pair (usually located in ~/.ssh) and copy its content.\nPaste the content into the “Key” field on GitHub and give the key a descriptive title (e.g., “Personal GitHub Key”).\nClick “Add SSH key” to save it.\n\nRepeat the same process for your professional GitHub account, adding the public key from the other SSH key pair.\nConfigure SSH Config File: To make sure your SSH client uses the correct key for each GitHub account, you can create or edit your ~/.ssh/config file. Open the file in a text editor and add the following configuration:\n# Personal GitHub account\nHost github.com-personal\nHostName github.com\nUser git\nIdentityFile ~/.ssh/id_ed25519\n\n# Professional GitHub account\nHost github.com-professional\nHostName github.com\nUser git\nIdentityFile ~/.ssh/other_id_ed25519\nReplace ~/.ssh/id_ed25519 with the path to your personal private key and ~/.ssh/other_id_ed25519 with the path to your professional private key.\nUse SSH URLs for Repositories: Now, when we want to clone or interact with repositories associated with each GitHub account, we use SSH URLs with the custom hosts you defined in the ~/.ssh/config file. For example:\n# Personal GitHub account repository\ngit clone git@github.com-personal:user/repo.git\n\n# Professional GitHub account repository\ngit clone git@github.com-professional:user/repo.git\n\nWith these steps, we should be able to use separate SSH keys for your personal and professional GitHub accounts on your local desktop. The ~/.ssh/config file will help your SSH client select the appropriate key based on the custom hosts you specified for each account."
  }
]